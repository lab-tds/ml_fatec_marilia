# -*- coding: utf-8 -*-
"""us_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AADUoAtzIGMzEMJznfb6XrUtQ8GFAK6D
"""

from google.colab import files

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler

uploaded = files.upload()

!ls -l

df = pd.read_csv('BUSBRA_4_ml.csv')
df.head()

y = df['Pathology']
X = df.drop(columns=['ID','Pathology'])

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

clf = LogisticRegression(random_state=0, max_iter=1000)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(cm).plot()

print(classification_report(y_test, y_pred, zero_division=0))

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.neural_network import MLPClassifier

clfs = {
    'DecisionTreeClassifier': DecisionTreeClassifier(),
    'RandomForestClassifier': RandomForestClassifier(),
    'XGBoost': xgb.XGBClassifier(),
    'MLP': MLPClassifier(max_iter=1000),
}

for key in clfs:
  clf = clfs[key]
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(key)
  print(classification_report(y_test, y_pred, zero_division=0))

from sklearn.model_selection import cross_val_score

for key in clfs:
  scores = cross_val_score(clfs[key], X, y, cv=5)
  print("%s: %0.2f accuracy with a standard deviation of %0.2f" % (key, scores.mean(), scores.std()))

from sklearn.model_selection import GridSearchCV

param_test = {
 'max_depth':range(3,15,2),
 'min_child_weight':range(1,10,2),
  'gamma':[i/10.0 for i in range(0,5)],
#  'subsample':[i/10.0 for i in range(6,10)],
#  'colsample_bytree':[i/10.0 for i in range(6,10)],
}

gsearch = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,
                                                     min_child_weight=2,
                                                     gamma=0, subsample=0.8,
                                                     colsample_bytree=0.8,
                                                     objective= 'binary:logistic',
                                                     nthread=4, scale_pos_weight=1,seed=27),
                       param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)
gsearch.fit(X_train,y_train)
gsearch.best_params_, gsearch.best_score_

import keras
from keras import layers
from tensorflow import data as tf_data
import matplotlib.pyplot as plt

!wget http://cancermamabrasil.com.br/BUSBRA_4_dl.zip

!unzip -qq BUSBRA_4_dl.zip

!ls -l US/

SIZE = 128
image_size = (SIZE, SIZE)
batch_size = 64

train_ds, val_ds = keras.utils.image_dataset_from_directory(
    "US",
    validation_split=0.25,
    subset="both",
    seed=133,
    color_mode="grayscale",
    image_size=image_size,
    batch_size=batch_size,
)

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(np.array(images[i]).astype("uint8"), cmap='gray')
        plt.title(int(labels[i]))
        plt.axis("off")

plt.figure(figsize=(10, 10))
for images, labels in val_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(np.array(images[i]).astype("uint8"), cmap='gray')
        plt.title(int(labels[i]))
        plt.axis("off")

data_augmentation_layers = [
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
]


def data_augmentation(images):
    for layer in data_augmentation_layers:
        images = layer(images)
    return images

# data_augmentation
train_ds = train_ds.map(
    lambda img, label: (data_augmentation(img), label),
    num_parallel_calls=tf_data.AUTOTUNE,
)
# Prefetching samples in GPU memory helps maximize GPU utilization.
train_ds = train_ds.prefetch(tf_data.AUTOTUNE)
val_ds = val_ds.prefetch(tf_data.AUTOTUNE)

model = keras.Sequential([
    layers.Input(shape=(SIZE, SIZE, 1)),  # Input layer for 28x28 grayscale images
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(2, activation='softmax') # Output layer for 10 classes
])
keras.utils.plot_model(model, show_shapes=True)

epochs = 25

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(
    train_ds,
    epochs=epochs,
    validation_data=val_ds,
)

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

img = keras.utils.load_img("US/Malignant/bus_0015-s.png", target_size=image_size, color_mode='grayscale')
plt.imshow(img, cmap='gray')

img_array = keras.utils.img_to_array(img)
img_array = img_array
img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis

predictions = model.predict(img_array)
print(predictions[0][1])
print(f"This image is {100 * predictions[0][0]:.2f}% Benigno and {100 * predictions[0][1]:.2f}% Maligno.")

img = keras.utils.load_img("US/Benign/bus_0008-r.png", target_size=image_size, color_mode='grayscale')
plt.imshow(img, cmap='gray')

img_array = keras.utils.img_to_array(img)
img_array = img_array
img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis

predictions = model.predict(img_array)
print(predictions[0][1])
print(f"This image is {100 * predictions[0][0]:.2f}% Benigno and {100 * predictions[0][1]:.2f}% Maligno.")